{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "from torch import nn\n",
    "from equiformer_pytorch import Equiformer\n",
    "from data import PDB_DATASET, load_case_list\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = pickle.load(open(\"\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMS_predictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMS_predictor, self).__init__()\n",
    "        self.model = Equiformer(\n",
    "            num_tokens = len(['N', 'O', 'P', 'C', 'S', 'F', 'CL', 'BR']),\n",
    "            dim = (16, 8, 8),               # dimensions per type, ascending, length must match number of degrees (num_degrees)\n",
    "            dim_head = (16, 8, 8),          # dimension per attention head\n",
    "            heads = (1, 1, 1),             # number of attention heads\n",
    "            num_degrees = 3,               # number of degrees\n",
    "            depth = 3,                     # depth of equivariant transformer\n",
    "            attend_self = True,            # attending to self or not\n",
    "            reduce_dim_out = False,         # whether to reduce out to dimension of 1, say for predicting new coordinates for type 1 features\n",
    "            dot_product_attention = True,  # set to False to try out MLP attention\n",
    "            num_neighbors = 42\n",
    "        )\n",
    "        self.mlp_1 = nn.Linear(16, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mlp_2 = nn.Linear(256, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, feature, coord, mask=None):\n",
    "        x = self.model(feature, coord, mask).type0\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.mlp_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.mlp_2(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdb(pdb_path):\n",
    "    element_map = {e: i for i, e in enumerate(['N', 'O', 'P', 'C', 'S', 'F', 'CL', 'BR'])}\n",
    "    with open(pdb_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    coord = []\n",
    "    ele = []\n",
    "    score = None\n",
    "    atom_count = 0\n",
    "    for line in lines:\n",
    "        if line[:4] == 'ATOM' or line[:6] == \"HETATM\":\n",
    "            atom_ele = line[76:78].strip().upper()\n",
    "            if atom_ele != 'H' and atom_ele != 'Z': # exlude hydrogen and dummy atoms\n",
    "                x = float(line[30:38])\n",
    "                y = float(line[38:46])\n",
    "                z = float(line[46:54])\n",
    "                atom_coord = torch.tensor([x, y, z])\n",
    "                ele.append(element_map[atom_ele])\n",
    "                coord.append(atom_coord)\n",
    "                atom_count += 1\n",
    "        if line[:4] == 'rms ':  # not rms_stem\n",
    "            score = torch.tensor(float(line[4:].strip()))\n",
    "    coord = torch.stack(coord)\n",
    "    ele = torch.tensor(ele)\n",
    "    # if score is None:\n",
    "    #     raise ValueError(\"No score found in pdb file\")\n",
    "    return {\"cord\": coord, \"ele\": ele, \"path\": pdb_path, \"rms\": score,\n",
    "            \"num_atom\": atom_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"\"\n",
    "\n",
    "model = RMS_predictor().cuda()  \n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "\n",
    "for rna in eval_dict:\n",
    "    if rna in [\"6cb3\", \"3f30\", \"3f2w\", \"4k32\", \"7edl\", \"2yie\", \"4p3s\"]:\n",
    "        continue\n",
    "    res_dict[rna] = {}\n",
    "    print(rna)\n",
    "    for gt in eval_dict[rna]:\n",
    "        res_dict[rna][gt] = {}\n",
    "        for ligand in eval_dict[rna][gt]:\n",
    "            structure_path = f\"\"\n",
    "            input_data = parse_pdb(structure_path)\n",
    "            ele = input_data['ele'].cuda().unsqueeze(0)\n",
    "            cord = input_data['cord'].cuda().unsqueeze(0)\n",
    "            pred_rms = model(ele, cord).item()\n",
    "            res_dict[rna][gt][ligand] = pred_rms\n",
    "        # print(res_dict)\n",
    "    #     break\n",
    "        \n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_list = []\n",
    "for rna in res_dict:\n",
    "    for gt in res_dict[rna]:\n",
    "        ligand_name_list = []\n",
    "        liagnd_rms_list = []\n",
    "        for ligand in res_dict[rna][gt]:\n",
    "            print(rna, gt, ligand, res_dict[rna][gt][ligand])\n",
    "            liagnd_rms_list.append(res_dict[rna][gt][ligand])\n",
    "            ligand_name_list.append(ligand)\n",
    "        sorted_name_list = [x for _, x in sorted(zip(liagnd_rms_list, ligand_name_list))]\n",
    "        comp_list.append([gt, sorted_name_list])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for top_n in range(1, 11):\n",
    "    count = 0\n",
    "    for pair_list in comp_list:\n",
    "        if pair_list[0] in pair_list[1][:top_n]:\n",
    "            count += 1\n",
    "    res[top_n] = (count / len(comp_list))\n",
    "                #   (len(comp_list) - count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f1467373701e29948540d6f6f33ad8e4e82bbd273a2b0edd83469f605f50d5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
